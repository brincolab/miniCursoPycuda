{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __CUDA__ __C__  _#_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://docs.nvidia.com/cuda\"><img src=\"imagen/CUDA.png\" width=\"30%\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo básico: Suma de vectores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Recordemos como se ve un programa de C. Posteriormente el entorno puro de CUDA C.\n",
    "Como veremos a continuacion realmente programar la targeta grafica se reduce en primera instancia a tener un codigo en C# y agregar sobre este funciones de propias de CUDA (o *kernels*). Estas funciones o _kernels_ son ejecutados en la GPU de forma paralela.\n",
    "\n",
    ">En este primer ejemplo es evidente la manera de paralelizar la suma de vectores\n",
    "![Alt text](imagen/suma.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "#include <stdio.h>\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "int N = 10;\n",
    "float a[N],b[N],c[N];\n",
    "\n",
    "for (int i = 0; i < N; ++i){\n",
    "\ta[i] = i;\n",
    "\tb[i] = 2.0f;\t\n",
    "}\n",
    "\n",
    "for (int i = 0; i < N; ++i){\n",
    "\tc[i]= a[i]+b[i];\t\n",
    "}\n",
    "\n",
    "for (int i = 0; i < N; ++i){\n",
    "\tprintf(\"%f \\n\",c[i]);\t\n",
    "}\n",
    "\n",
    "\n",
    "return 0;\n",
    "}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!g++ cpuAdd.c -o cpua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#include <stdio.h>\r\n",
      "\r\n",
      "int main(void)\r\n",
      "{\r\n",
      "int N = 10;\r\n",
      "float a[N],b[N],c[N];\r\n",
      "\r\n",
      "for (int i = 0; i < N; ++i){\r\n",
      "\ta[i] = i;\r\n",
      "\tb[i] = 2.0f;\t\r\n",
      "}\r\n",
      "\r\n",
      "for (int i = 0; i < N; ++i){\r\n",
      "\tc[i]= a[i]+b[i];\t\r\n",
      "}\r\n",
      "\r\n",
      "for (int i = 0; i < N; ++i){\r\n",
      "\tprintf(\"%f \\n\",c[i]);\t\r\n",
      "}\r\n",
      "\r\n",
      "\r\n",
      "return 0;\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!cat cpuAdd.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.000000 \r\n",
      "3.000000 \r\n",
      "4.000000 \r\n",
      "5.000000 \r\n",
      "6.000000 \r\n",
      "7.000000 \r\n",
      "8.000000 \r\n",
      "9.000000 \r\n",
      "10.000000 \r\n",
      "11.000000 \r\n"
     ]
    }
   ],
   "source": [
    "!./cpua"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version CUDA C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](imagen/cuda3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](imagen/CUDAmodelThreads.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "// CUDA Kernel\n",
    "__global__ void vectorAdd(const float *A, const float *B, float *C, int numElements)\n",
    "{\n",
    "    int i = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    if (i < numElements)\n",
    "    {\n",
    "        C[i] = A[i] + B[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "/**\n",
    " * Host main routine\n",
    " */\n",
    "int main(void)\n",
    "{\n",
    "    int numElements = 15;\n",
    "    size_t size = numElements * sizeof(float);\n",
    "    printf(\"[Vector addition of %d elements]\\n\", numElements);\n",
    "\n",
    "    float a[numElements],b[numElements],c[numElements];\n",
    "    float *a_gpu,*b_gpu,*c_gpu;\n",
    "\n",
    "    cudaMalloc((void **)&a_gpu, size);\n",
    "    cudaMalloc((void **)&b_gpu, size);\n",
    "    cudaMalloc((void **)&c_gpu, size);\n",
    "\n",
    "    for (int i=0;i<numElements;++i ){\n",
    "    \n",
    "    \ta[i] = i*i;\n",
    "    \tb[i] = i;\n",
    "    \n",
    "    }\n",
    "    // Copy the host input vectors A and B in host memory to the device input vectors in\n",
    "    // device memory\n",
    "    printf(\"Copy input data from the host memory to the CUDA device\\n\");\n",
    "    cudaMemcpy(a_gpu, a, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(b_gpu, b, size, cudaMemcpyHostToDevice);\n",
    "\n",
    "    // Launch the Vector Add CUDA Kernel\n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid =(numElements + threadsPerBlock - 1) / threadsPerBlock;\n",
    "    printf(\"CUDA kernel launch with %d blocks of %d threads\\n\", blocksPerGrid, threadsPerBlock);\n",
    "    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(a_gpu, b_gpu, c_gpu, numElements);\n",
    "\n",
    "    // Copy the device result vector in device memory to the host result vector\n",
    "    // in host memory.\n",
    "    printf(\"Copy output data from the CUDA device to the host memory\\n\");\n",
    "    cudaMemcpy(c, c_gpu, size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    for (int i=0;i<numElements;++i ){\n",
    "    \tprintf(\"%f \\n\",c[i]);\n",
    "    }\n",
    "\n",
    "    // Free device global memory\n",
    "    cudaFree(a_gpu);\n",
    "    cudaFree(b_gpu);\n",
    "    cudaFree(c_gpu);\n",
    "    \n",
    "    printf(\"Done\\n\");\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!nvcc gpuAdd.cu -o gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Vector addition of 15 elements]\n",
      "Copy input data from the host memory to the CUDA device\n",
      "CUDA kernel launch with 1 blocks of 256 threads\n",
      "Copy output data from the CUDA device to the host memory\n",
      "0.000000 \n",
      "2.000000 \n",
      "6.000000 \n",
      "12.000000 \n",
      "20.000000 \n",
      "30.000000 \n",
      "42.000000 \n",
      "56.000000 \n",
      "72.000000 \n",
      "90.000000 \n",
      "110.000000 \n",
      "132.000000 \n",
      "156.000000 \n",
      "182.000000 \n",
      "210.000000 \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "!./gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep 14 18:01:19 2015       \r\n",
      "+------------------------------------------------------+                       \r\n",
      "| NVIDIA-SMI 346.46     Driver Version: 346.46         |                       \r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 780     Off  | 0000:01:00.0     N/A |                  N/A |\r\n",
      "| 26%   37C    P0    N/A /  N/A |    572MiB /  3071MiB |     N/A      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0            C+G   Not Supported                                         |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1ra Implementación de PyCUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pycuda import autoinit\n",
    "from pycuda import gpuarray\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aux = range(15)\n",
    "a = np.array(aux).astype(np.float32)\n",
    "b = (a*a).astype(np.float32)\n",
    "c = np.zeros(len(aux)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_gpu = gpuarray.to_gpu(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b_gpu = gpuarray.to_gpu(b)\n",
    "c_gpu = gpuarray.to_gpu(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aux_gpu = a_gpu+b_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pycuda._driver.DeviceAllocation at 0x7f4d09f78670>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux_gpu.gpudata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pycuda.gpuarray.GPUArray"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(aux_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
       "         11.,  12.,  13.,  14.], dtype=float32),\n",
       " array([   0.,    1.,    4.,    9.,   16.,   25.,   36.,   49.,   64.,\n",
       "          81.,  100.,  121.,  144.,  169.,  196.], dtype=float32),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.], dtype=float32))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_gpu,b_gpu,c_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2da Implementación de PyCUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pycuda.elementwise import ElementwiseKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_gpu.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myCudaFunc = ElementwiseKernel(arguments = \"float *a, float *b, float *c\",\n",
    "                               operation = \"c[i] = a[i]+b[i]\",\n",
    "                               name = \"mySumK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myCudaFunc(a_gpu,b_gpu,c_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.,    2.,    6.,   12.,   20.,   30.,   42.,   56.,   72.,\n",
       "         90.,  110.,  132.,  156.,  182.,  210.], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3ra Implementación de PyCUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pycuda.compiler import SourceModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cudaCode = open(\"gpuAdd.cu\",\"r\")\n",
    "myCUDACode = cudaCode.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:1: UserWarning: The CUDA compiler succeeded, but said the following:\n",
      "kernel.cu(20): warning: linkage specification is not allowed\n",
      "\n",
      "kernel.cu(20): warning: linkage specification is not allowed\n",
      "\n",
      "\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "myCode = SourceModule(myCUDACode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importedKernel = myCode.get_function(\"vectorAdd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_Simple\t       README.md\r\n",
      "addCPU\t\t       README.md~\r\n",
      "Animation.ipynb        SchrodingerBOX.pdf\r\n",
      "cpua\t\t       Sesion1_Introduccion_Python.ipynb\r\n",
      "cpuAdd.c\t       Sesion1_Test.ipynb\r\n",
      "cpuAdd.c~\t       Sesion2_Intento_de_solucion_GPE.ipynb\r\n",
      "CUDAkernelsCONS.cu     Sesión3.1_ExploreGPU.ipynb\r\n",
      "CUDAkernelsCONS.cu~    Sesion3_CUDA&PyCuda1st.ipynb\r\n",
      "CUDAkernels.cu\t       Sesion4_PyCUDA_2nd.ipynb\r\n",
      "CUDAkernels.cu~        Sesion5_MemoriasCUDA.ipynb\r\n",
      "gpu\t\t       Sesion6_PDE_CUDA_670.ipynb\r\n",
      "gpuAdd\t\t       Sesion6_PDE_CUDA_780.ipynb\r\n",
      "gpuAdd.cu\t       Sesion6_PDE_CUDA.ipynb\r\n",
      "gpuAdd.cu~\t       Sesion6_PDE_CUDA-Tesla.ipynb\r\n",
      "imagen\t\t       Sesion7_LibreriasGPU.ipynb\r\n",
      "intervalo.py\t       Sesion8_PDE_CUDA_Schrodinger.ipynb\r\n",
      "intervalo.pyc\t       Some_bench.ipynb\r\n",
      "libpeerconnection.log  taylor.py\r\n",
      "License.txt\t       taylor.py~\r\n",
      "Longuet-Higgins.ipynb  taylor.pyc\r\n",
      "N-Body.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nData = len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nThreadsPerBlock = 256\n",
    "nBlockPerGrid = 1\n",
    "nGridsPerBlock = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_gpu.set(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importedKernel(a_gpu.gpudata,b_gpu.gpudata,c_gpu.gpudata,block=(256,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.,    2.,    6.,   12.,   20.,   30.,   42.,   56.,   72.,\n",
       "         90.,  110.,  132.,  156.,  182.,  210.], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde una perspectiva de cálculo científico podemos resumir a este punto lo siguiente:\n",
    "\n",
    "**Kernel**\n",
    ">La funcion elemental de parelización es conocido como *Kernel* de CUDA, se pueden imaginar a estas fucniones escritas con sintaxis de C# como las funciones que ejecutaran en paralelo en cada procesador en los miles que estan dentro de la GPU.\n",
    "\n",
    "> __global__\n",
    "> __device__\n",
    "> __host__\n",
    "\n",
    "**¿En donde esta la paralelización?**\n",
    ">Cada vez que se llama a un kernel es necesarion darle una distribucion de hilos (o _threads_) los cuales se organizan en bloques (_blocks_) y estos a su vez en un _grid_ (estos puden poseer distintas dimensiones: 1D,2D,3D) . Estos threads son copias del kernel y cada uno es un proceso a llevarse a cabo en la GPU, es decir si por ejemplo lanzamos un grid con 5 bloques (_gridDim_= (5,1,1)) con 10 threads por bloque (_blockDim_ = (10,1,1)), entonces habremos lanzado 50 tareas en paralelo.\n",
    "Si bien los kernels a ejecutar por los thread son copias del que escribimos originalmente, la diferenciación se da mediante el asignamiento de un contador a cada proceso, la manera usual de determinar este **indice de proceso global** se ejemplifica asi:\n",
    "![Alt text](imagen/CUDAmodelThreads.png)\n",
    "\n",
    ">(**ojo** este pude cambiar dependiendo de la dimension de bloques y de threads)\n",
    "![Alt text](imagen/cuda-grid.png)\n",
    "\n",
    ">Para nuestro ejemplo de suma de vectores hemos usado el **indice de proceso global** para que cada thread realice la suma sobre una componente distinta de los vectores. Es en este punto donde aparece la paralelizacion, ya que cada thread sumo una componente distinta del vector.\n",
    "\n",
    "**PyCUDA**\n",
    ">Esta biblioteca de python surge como un desarrollo que nos permite en principio hacer todo lo que podemos hacer con CUDA C de una manera más sencilla. Esta biblioteca tiene distintos niveles de implementacion que nos permite desde usar Kernels de CUDA C, hasta no usar ningun kernel explicitamente.\n",
    ">Una de las virtudes que hemos visto es el uso de la clase **GPUArray** la cual nos permite gestionar de manera sencilla la memoria, asignacion de valores, estructura, transferencias de datos, etc. entre CPU y GPU. Esta clase de pyCUDA mantiene la estructura de arreglos de la biblioteca **numpy** y de manera natural se extienden muchas de las fuciones.\n",
    "\n",
    ">Una vez inicializado algun contexto de pyCUDA podemos hacer uso de la clase GPUArray, la manera mas simple de generar un arreglo en la memoria global de la GPU es mediante _gpuarray.to_gpu()_ en donde el valor que se pasa a la funcion es un arreglo de **numpy**. Aunque todos los arreglos en memoria global de GPU son arreglos lineales, la clase GPUArray maneja la posibilidad de preservar las dimensiones del arreglo. Desde un Kernel siempre que accedamos a nuestros datos sera de manera lineal (es decir, todos los datos son a lo mas vectores), por lo que el arreglo de arreglos (matrices o mallas 3D) no es posible utilizarlo en memoria global de manera natrural."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#<a href=\"http://documen.tician.de/pycuda/\">pyCUDA</a>\n",
    "\n",
    "#<a href=\"http://docs.scipy.org/doc/numpy/reference/\">Numpy</a>\n",
    "\n",
    "#<a href=\"http://docs.nvidia.com/cuda\">CUDA</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
